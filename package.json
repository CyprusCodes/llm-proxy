{
  "name": "llm-proxy",
  "version": "1.2.0",
  "description": "An LLM Proxy that allows the user to interact with different language models from different providers using unified request and response formats.",
  "main": "dist/index.js",
  "types": "dist/index.d.ts",
  "files": [
    "dist/**/*"
  ],
  "scripts": {
    "build": "tsc",
    "prepare": "npm run build",
    "test": "jest --config jest.config.js",
    "dev": "ts-node src/index.ts"
  },
  "keywords": [],
  "author": "Ahmad Jawabreh - jawabreh0",
  "license": "MIT",
  "devDependencies": {
    "@types/dotenv": "^8.2.3",
    "@types/jest": "^29.5.14",
    "@types/node": "^22.8.6",
    "jest": "^29.7.0",
    "ts-jest": "^29.2.5",
    "ts-node": "^10.9.2",
    "typescript": "^5.6.3"
  },
  "dependencies": {
    "@anthropic-ai/bedrock-sdk": "^0.11.1",
    "aws-sdk": "^2.1691.0",
    "axios": "^1.7.7",
    "dotenv": "^16.4.5",
    "llm-proxy": "^1.2.0",
    "openai": "^4.69.0"
  }
}
